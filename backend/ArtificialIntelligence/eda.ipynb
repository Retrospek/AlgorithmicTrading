{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import nasdaqdatalink\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,\n",
    "    VotingRegressor, StackingRegressor, BaggingRegressor\n",
    ")\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--- Starting Here ---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ON', 'MRVL', 'ASML', 'AMD', 'AVGO', 'TSM', 'NVDA']\n",
      "TSM rows: 3509\n",
      "AVGO rows: 3509\n",
      "AMD rows: 3509\n",
      "ASML rows: 3509\n",
      "MRVL rows: 3509\n",
      "ON rows: 3509\n",
      "NVDA rows: 3509\n",
      "Index(['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'Ticker', 'RSI', 'RSI_Scaled'], dtype='object')\n",
      "Close         float64\n",
      "High          float64\n",
      "Low           float64\n",
      "Open          float64\n",
      "Volume          int64\n",
      "Ticker         object\n",
      "RSI           float64\n",
      "RSI_Scaled    float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.67</td>\n",
       "      <td>8.06</td>\n",
       "      <td>7.62</td>\n",
       "      <td>8.06</td>\n",
       "      <td>13017500</td>\n",
       "      <td>ON</td>\n",
       "      <td>21.962626</td>\n",
       "      <td>0.179946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.92</td>\n",
       "      <td>8.02</td>\n",
       "      <td>7.74</td>\n",
       "      <td>7.76</td>\n",
       "      <td>10853300</td>\n",
       "      <td>ON</td>\n",
       "      <td>30.125530</td>\n",
       "      <td>0.265726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.88</td>\n",
       "      <td>7.96</td>\n",
       "      <td>7.74</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7781900</td>\n",
       "      <td>ON</td>\n",
       "      <td>30.638304</td>\n",
       "      <td>0.271114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.94</td>\n",
       "      <td>8.01</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.88</td>\n",
       "      <td>8462600</td>\n",
       "      <td>ON</td>\n",
       "      <td>29.130450</td>\n",
       "      <td>0.255269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.44</td>\n",
       "      <td>7.99</td>\n",
       "      <td>7.29</td>\n",
       "      <td>7.99</td>\n",
       "      <td>20256200</td>\n",
       "      <td>ON</td>\n",
       "      <td>24.014345</td>\n",
       "      <td>0.201507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Close  High   Low  Open    Volume Ticker        RSI  RSI_Scaled\n",
       "13   7.67  8.06  7.62  8.06  13017500     ON  21.962626    0.179946\n",
       "14   7.92  8.02  7.74  7.76  10853300     ON  30.125530    0.265726\n",
       "15   7.88  7.96  7.74  7.85   7781900     ON  30.638304    0.271114\n",
       "16   7.94  8.01  7.75  7.88   8462600     ON  29.130450    0.255269\n",
       "17   7.44  7.99  7.29  7.99  20256200     ON  24.014345    0.201507"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NVDA = 'NVDA'\n",
    "SEMICONDUCTORS = [\"TSM\", \"AVGO\", \"AMD\", \"ASML\", \"MRVL\", \"ON\"]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # Scaling RSI values for more relatable trends\n",
    "\n",
    "semiConductor_data = yf.download(NVDA, start='2010-01-01', end='2024-01-01')\n",
    "semiConductor_data.columns = [col[0] for col in semiConductor_data.columns]\n",
    "semiConductor_data['Ticker'] = NVDA\n",
    "semiConductor_data['RSI'] = calculate_rsi(semiConductor_data['Close'], window=14)\n",
    "semiConductor_data['RSI_Scaled'] = scaler.fit_transform(semiConductor_data['RSI'].values.reshape(-1, 1))\n",
    "semiConductor_data.reset_index(inplace=True)\n",
    "semiConductor_data = semiConductor_data[['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'Ticker', 'RSI', 'RSI_Scaled']]\n",
    "\n",
    "#print(f\"Total Data Length: {len(semiConductor_data)}\")\n",
    "\n",
    "for stock in SEMICONDUCTORS:\n",
    "    stock_data = yf.download(stock, start=\"2010-01-01\", end=\"2024-01-01\")\n",
    "\n",
    "    stock_data['Ticker'] = stock\n",
    "    stock_data['RSI'] = calculate_rsi(stock_data['Close'], window=14)\n",
    "    stock_data['RSI_Scaled'] = scaler.fit_transform(stock_data['RSI'].values.reshape(-1, 1))\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    #stock_data = stock_data[['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'Ticker', 'RSI', 'RSI_Scaled']]\n",
    "    stock_data.columns = [col[0] for col in stock_data.columns]\n",
    "\n",
    "    semiConductor_data = pd.concat([stock_data, semiConductor_data], ignore_index=True)\n",
    "\n",
    "    #print([val[0] for val in semiConductor_data.columns.tolist()])\n",
    "    #print(f\"Total Data Length: {len(semiConductor_data)}\")\n",
    "\n",
    "semiConductor_data = semiConductor_data.dropna()\n",
    "semiConductor_data.to_csv(\"semiconductorData.csv\", index=False)\n",
    "print([ i for i in semiConductor_data['Ticker'].unique()])\n",
    "for stock in SEMICONDUCTORS + [NVDA]:\n",
    "    print(f\"{stock} rows: {len(semiConductor_data.loc[semiConductor_data['Ticker'] == stock])}\")\n",
    "\n",
    "print(semiConductor_data.columns)\n",
    "semiConductor_data.drop(columns=['Date'], inplace=True)\n",
    "print(semiConductor_data.dtypes)\n",
    "semiConductor_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstmDataset(Dataset):\n",
    "    def __init__(self, data, features, targets, input_window=15, output_window=10):\n",
    "        self.data = torch.FloatTensor(data.values) # 2 dimensional array with each array being all the features in a data point\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples that can be generated.\n",
    "        return len(self.data) - self.input_window - self.output_window + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #Generates a single sample of data.\n",
    "        #Args:\n",
    "        #    idx (int): Index for the sample.\n",
    "        #Returns:\n",
    "        #    Tuple[torch.Tensor, torch.Tensor]: Input and output tensors.\n",
    "        x = self.data[idx : idx + self.input_window] # Gets the 15 values we're using for predictions\n",
    "        y = self.data[idx + self.input_window : idx + self.input_window + self.output_window][7] # Gets the 10 values we're trying to predict\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ON': <__main__.lstmDataset object at 0x000001D511877860>, 'MRVL': <__main__.lstmDataset object at 0x000001D50EDE4830>, 'ASML': <__main__.lstmDataset object at 0x000001D5118AE090>, 'AMD': <__main__.lstmDataset object at 0x000001D5118ACE90>, 'AVGO': <__main__.lstmDataset object at 0x000001D50FB2C680>, 'TSM': <__main__.lstmDataset object at 0x000001D5118ACAA0>, 'NVDA': <__main__.lstmDataset object at 0x000001D5118AE210>}\n",
      "{'ON': <torch.utils.data.dataloader.DataLoader object at 0x000001D511A07FE0>, 'MRVL': <torch.utils.data.dataloader.DataLoader object at 0x000001D5118744A0>, 'ASML': <torch.utils.data.dataloader.DataLoader object at 0x000001D51189CE60>, 'AMD': <torch.utils.data.dataloader.DataLoader object at 0x000001D50FA56690>, 'AVGO': <torch.utils.data.dataloader.DataLoader object at 0x000001D5118AF8F0>, 'TSM': <torch.utils.data.dataloader.DataLoader object at 0x000001D5118AA7E0>, 'NVDA': <torch.utils.data.dataloader.DataLoader object at 0x000001D5118AB290>}\n"
     ]
    }
   ],
   "source": [
    "company_datasets = {}\n",
    "tickers = semiConductor_data['Ticker'].unique()\n",
    "\n",
    "# Create an lstmDataset for each company and store it in company_datasets\n",
    "for ticker in tickers:\n",
    "    data = semiConductor_data[semiConductor_data['Ticker'] == ticker].drop(columns=[\"Ticker\"])\n",
    "    company_datasets[ticker] = lstmDataset(data, input_window=15, output_window=10, features=features, targets=targets)\n",
    "print(company_datasets)\n",
    "\n",
    "batch_size = 32 \n",
    "company_loaders = {}\n",
    "\n",
    "for ticker, dataset in company_datasets.items():\n",
    "    company_loaders[ticker] = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(company_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout=0.0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        layer1 = 64\n",
    "        layer2 = 32\n",
    "        layer3 = 16\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=layer1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(input_size=layer1, \n",
    "                             hidden_size=layer2, \n",
    "                             batch_first=True, \n",
    "                             dropout=0)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(input_size=layer2, \n",
    "                             hidden_size=layer3, \n",
    "                             batch_first=True, \n",
    "                             dropout=0)\n",
    "        \n",
    "        self.fc = nn.Linear(layer3, output_size)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out = out[:, -1, :]  # Shape: [batch_size, hidden_size]\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 7])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: ON, Epoch: 0, Batch: 0, Loss: 7440984178688.0\n",
      "Company: ON, Epoch: 0, Batch: 100, Loss: 12970950131712.0\n",
      "Company: ON, Epoch: 1, Batch: 0, Loss: 7114153000960.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([29, 7])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: ON, Epoch: 1, Batch: 100, Loss: 5973621604352.0\n",
      "Company: ON, Epoch: 2, Batch: 0, Loss: 9461130330112.0\n",
      "Company: ON, Epoch: 2, Batch: 100, Loss: 7222464610304.0\n",
      "Company: ON, Epoch: 3, Batch: 0, Loss: 12177520984064.0\n",
      "Company: ON, Epoch: 3, Batch: 100, Loss: 8863424184320.0\n",
      "Company: ON, Epoch: 4, Batch: 0, Loss: 4672551649280.0\n",
      "Company: ON, Epoch: 4, Batch: 100, Loss: 8672662519808.0\n",
      "Company: ON, Epoch: 5, Batch: 0, Loss: 8911103983616.0\n",
      "Company: ON, Epoch: 5, Batch: 100, Loss: 7785534193664.0\n",
      "Company: ON, Epoch: 6, Batch: 0, Loss: 6677825323008.0\n",
      "Company: ON, Epoch: 6, Batch: 100, Loss: 9719448076288.0\n",
      "Company: ON, Epoch: 7, Batch: 0, Loss: 14293046132736.0\n",
      "Company: ON, Epoch: 7, Batch: 100, Loss: 11754604068864.0\n",
      "Company: ON, Epoch: 8, Batch: 0, Loss: 6947685793792.0\n",
      "Company: ON, Epoch: 8, Batch: 100, Loss: 8185527140352.0\n",
      "Company: ON, Epoch: 9, Batch: 0, Loss: 7865724567552.0\n",
      "Company: ON, Epoch: 9, Batch: 100, Loss: 8061258825728.0\n",
      "Company: MRVL, Epoch: 0, Batch: 0, Loss: 32085941157888.0\n",
      "Company: MRVL, Epoch: 0, Batch: 100, Loss: 13829569249280.0\n",
      "Company: MRVL, Epoch: 1, Batch: 0, Loss: 20012607733760.0\n",
      "Company: MRVL, Epoch: 1, Batch: 100, Loss: 15433967075328.0\n",
      "Company: MRVL, Epoch: 2, Batch: 0, Loss: 18681333547008.0\n",
      "Company: MRVL, Epoch: 2, Batch: 100, Loss: 18712170070016.0\n",
      "Company: MRVL, Epoch: 3, Batch: 0, Loss: 18518181412864.0\n",
      "Company: MRVL, Epoch: 3, Batch: 100, Loss: 18477167411200.0\n",
      "Company: MRVL, Epoch: 4, Batch: 0, Loss: 33376549470208.0\n",
      "Company: MRVL, Epoch: 4, Batch: 100, Loss: 16739573170176.0\n",
      "Company: MRVL, Epoch: 5, Batch: 0, Loss: 14595475374080.0\n",
      "Company: MRVL, Epoch: 5, Batch: 100, Loss: 15428989485056.0\n",
      "Company: MRVL, Epoch: 6, Batch: 0, Loss: 18431745196032.0\n",
      "Company: MRVL, Epoch: 6, Batch: 100, Loss: 9008648814592.0\n",
      "Company: MRVL, Epoch: 7, Batch: 0, Loss: 17250167816192.0\n",
      "Company: MRVL, Epoch: 7, Batch: 100, Loss: 18638513897472.0\n",
      "Company: MRVL, Epoch: 8, Batch: 0, Loss: 9815469326336.0\n",
      "Company: MRVL, Epoch: 8, Batch: 100, Loss: 20764052946944.0\n",
      "Company: MRVL, Epoch: 9, Batch: 0, Loss: 19409926094848.0\n",
      "Company: MRVL, Epoch: 9, Batch: 100, Loss: 12104961622016.0\n",
      "Company: ASML, Epoch: 0, Batch: 0, Loss: 273568120832.0\n",
      "Company: ASML, Epoch: 0, Batch: 100, Loss: 590692417536.0\n",
      "Company: ASML, Epoch: 1, Batch: 0, Loss: 755890978816.0\n",
      "Company: ASML, Epoch: 1, Batch: 100, Loss: 390298337280.0\n",
      "Company: ASML, Epoch: 2, Batch: 0, Loss: 301353861120.0\n",
      "Company: ASML, Epoch: 2, Batch: 100, Loss: 292856430592.0\n",
      "Company: ASML, Epoch: 3, Batch: 0, Loss: 587543805952.0\n",
      "Company: ASML, Epoch: 3, Batch: 100, Loss: 303640707072.0\n",
      "Company: ASML, Epoch: 4, Batch: 0, Loss: 784095051776.0\n",
      "Company: ASML, Epoch: 4, Batch: 100, Loss: 363637702656.0\n",
      "Company: ASML, Epoch: 5, Batch: 0, Loss: 213850308608.0\n",
      "Company: ASML, Epoch: 5, Batch: 100, Loss: 1877957804032.0\n",
      "Company: ASML, Epoch: 6, Batch: 0, Loss: 405675900928.0\n",
      "Company: ASML, Epoch: 6, Batch: 100, Loss: 301253853184.0\n",
      "Company: ASML, Epoch: 7, Batch: 0, Loss: 259666083840.0\n",
      "Company: ASML, Epoch: 7, Batch: 100, Loss: 224778829824.0\n",
      "Company: ASML, Epoch: 8, Batch: 0, Loss: 595130318848.0\n",
      "Company: ASML, Epoch: 8, Batch: 100, Loss: 586351706112.0\n",
      "Company: ASML, Epoch: 9, Batch: 0, Loss: 404624867328.0\n",
      "Company: ASML, Epoch: 9, Batch: 100, Loss: 238506344448.0\n",
      "Company: AMD, Epoch: 0, Batch: 0, Loss: 575989675458560.0\n",
      "Company: AMD, Epoch: 0, Batch: 100, Loss: 511432659566592.0\n",
      "Company: AMD, Epoch: 1, Batch: 0, Loss: 467644125806592.0\n",
      "Company: AMD, Epoch: 1, Batch: 100, Loss: 444022510321664.0\n",
      "Company: AMD, Epoch: 2, Batch: 0, Loss: 522455223369728.0\n",
      "Company: AMD, Epoch: 2, Batch: 100, Loss: 774238923390976.0\n",
      "Company: AMD, Epoch: 3, Batch: 0, Loss: 391013587746816.0\n",
      "Company: AMD, Epoch: 3, Batch: 100, Loss: 525943508566016.0\n",
      "Company: AMD, Epoch: 4, Batch: 0, Loss: 667570659131392.0\n",
      "Company: AMD, Epoch: 4, Batch: 100, Loss: 449711899148288.0\n",
      "Company: AMD, Epoch: 5, Batch: 0, Loss: 262337911062528.0\n",
      "Company: AMD, Epoch: 5, Batch: 100, Loss: 725242607566848.0\n",
      "Company: AMD, Epoch: 6, Batch: 0, Loss: 817530717339648.0\n",
      "Company: AMD, Epoch: 6, Batch: 100, Loss: 765392028958720.0\n",
      "Company: AMD, Epoch: 7, Batch: 0, Loss: 533893795020800.0\n",
      "Company: AMD, Epoch: 7, Batch: 100, Loss: 548573590585344.0\n",
      "Company: AMD, Epoch: 8, Batch: 0, Loss: 354473247506432.0\n",
      "Company: AMD, Epoch: 8, Batch: 100, Loss: 490179081011200.0\n",
      "Company: AMD, Epoch: 9, Batch: 0, Loss: 457551221096448.0\n",
      "Company: AMD, Epoch: 9, Batch: 100, Loss: 437481543565312.0\n",
      "Company: AVGO, Epoch: 0, Batch: 0, Loss: 114658136555520.0\n",
      "Company: AVGO, Epoch: 0, Batch: 100, Loss: 96044134170624.0\n",
      "Company: AVGO, Epoch: 1, Batch: 0, Loss: 113215690244096.0\n",
      "Company: AVGO, Epoch: 1, Batch: 100, Loss: 1529891107373056.0\n",
      "Company: AVGO, Epoch: 2, Batch: 0, Loss: 97131339710464.0\n",
      "Company: AVGO, Epoch: 2, Batch: 100, Loss: 133885111304192.0\n",
      "Company: AVGO, Epoch: 3, Batch: 0, Loss: 75355352203264.0\n",
      "Company: AVGO, Epoch: 3, Batch: 100, Loss: 121691732705280.0\n",
      "Company: AVGO, Epoch: 4, Batch: 0, Loss: 140398051721216.0\n",
      "Company: AVGO, Epoch: 4, Batch: 100, Loss: 88537546358784.0\n",
      "Company: AVGO, Epoch: 5, Batch: 0, Loss: 83239091830784.0\n",
      "Company: AVGO, Epoch: 5, Batch: 100, Loss: 122187407163392.0\n",
      "Company: AVGO, Epoch: 6, Batch: 0, Loss: 94386738167808.0\n",
      "Company: AVGO, Epoch: 6, Batch: 100, Loss: 157777217454080.0\n",
      "Company: AVGO, Epoch: 7, Batch: 0, Loss: 119089494228992.0\n",
      "Company: AVGO, Epoch: 7, Batch: 100, Loss: 172352927170560.0\n",
      "Company: AVGO, Epoch: 8, Batch: 0, Loss: 86111586091008.0\n",
      "Company: AVGO, Epoch: 8, Batch: 100, Loss: 129462586834944.0\n",
      "Company: AVGO, Epoch: 9, Batch: 0, Loss: 98181291442176.0\n",
      "Company: AVGO, Epoch: 9, Batch: 100, Loss: 87440484204544.0\n",
      "Company: TSM, Epoch: 0, Batch: 0, Loss: 19027764183040.0\n",
      "Company: TSM, Epoch: 0, Batch: 100, Loss: 21820767993856.0\n",
      "Company: TSM, Epoch: 1, Batch: 0, Loss: 25397875965952.0\n",
      "Company: TSM, Epoch: 1, Batch: 100, Loss: 20335908880384.0\n",
      "Company: TSM, Epoch: 2, Batch: 0, Loss: 20305093328896.0\n",
      "Company: TSM, Epoch: 2, Batch: 100, Loss: 15993260736512.0\n",
      "Company: TSM, Epoch: 3, Batch: 0, Loss: 18730113302528.0\n",
      "Company: TSM, Epoch: 3, Batch: 100, Loss: 16766467047424.0\n",
      "Company: TSM, Epoch: 4, Batch: 0, Loss: 14764484853760.0\n",
      "Company: TSM, Epoch: 4, Batch: 100, Loss: 14953048178688.0\n",
      "Company: TSM, Epoch: 5, Batch: 0, Loss: 27606254616576.0\n",
      "Company: TSM, Epoch: 5, Batch: 100, Loss: 14735111094272.0\n",
      "Company: TSM, Epoch: 6, Batch: 0, Loss: 20684333907968.0\n",
      "Company: TSM, Epoch: 6, Batch: 100, Loss: 13562954121216.0\n",
      "Company: TSM, Epoch: 7, Batch: 0, Loss: 15129151275008.0\n",
      "Company: TSM, Epoch: 7, Batch: 100, Loss: 21731714531328.0\n",
      "Company: TSM, Epoch: 8, Batch: 0, Loss: 27189921710080.0\n",
      "Company: TSM, Epoch: 8, Batch: 100, Loss: 22866909200384.0\n",
      "Company: TSM, Epoch: 9, Batch: 0, Loss: 22267574616064.0\n",
      "Company: TSM, Epoch: 9, Batch: 100, Loss: 16120596660224.0\n",
      "Company: NVDA, Epoch: 0, Batch: 0, Loss: 8.19000871223296e+16\n",
      "Company: NVDA, Epoch: 0, Batch: 100, Loss: 5.649473279623168e+16\n",
      "Company: NVDA, Epoch: 1, Batch: 0, Loss: 7.854792527734374e+16\n",
      "Company: NVDA, Epoch: 1, Batch: 100, Loss: 5.570500856958157e+16\n",
      "Company: NVDA, Epoch: 2, Batch: 0, Loss: 3.870878789219123e+16\n",
      "Company: NVDA, Epoch: 2, Batch: 100, Loss: 4.936295248612557e+16\n",
      "Company: NVDA, Epoch: 3, Batch: 0, Loss: 4.427028520920678e+16\n",
      "Company: NVDA, Epoch: 3, Batch: 100, Loss: 6.276361552186573e+16\n",
      "Company: NVDA, Epoch: 4, Batch: 0, Loss: 4.419426428806758e+16\n",
      "Company: NVDA, Epoch: 4, Batch: 100, Loss: 2.940223156440269e+16\n",
      "Company: NVDA, Epoch: 5, Batch: 0, Loss: 4.461637367391846e+16\n",
      "Company: NVDA, Epoch: 5, Batch: 100, Loss: 1.0030464905091482e+17\n",
      "Company: NVDA, Epoch: 6, Batch: 0, Loss: 3.612398208417792e+16\n",
      "Company: NVDA, Epoch: 6, Batch: 100, Loss: 5.66602866056233e+16\n",
      "Company: NVDA, Epoch: 7, Batch: 0, Loss: 4.041111106486272e+16\n",
      "Company: NVDA, Epoch: 7, Batch: 100, Loss: 3.3948523166695424e+16\n",
      "Company: NVDA, Epoch: 8, Batch: 0, Loss: 6.254120923037696e+16\n",
      "Company: NVDA, Epoch: 8, Batch: 100, Loss: 5.520530200959386e+16\n",
      "Company: NVDA, Epoch: 9, Batch: 0, Loss: 3.956325445586125e+16\n",
      "Company: NVDA, Epoch: 9, Batch: 100, Loss: 5.87067311479849e+16\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  \n",
    "model = LSTMModel(input_size=7, output_size=1)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for ticker, loader in company_loaders.items():\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Company: {ticker}, Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 7])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss after 100 batches: 8854016119275.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([29, 7])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss after 200 batches: 12491147410145.28\n",
      "Test Loss after 300 batches: 9720238766271.146\n",
      "Test Loss after 400 batches: 97258323087728.64\n",
      "Test Loss after 500 batches: 130277352453603.33\n",
      "Test Loss after 600 batches: 123257163703391.58\n",
      "Test Loss after 700 batches: 3431192156118267.5\n",
      "Average Test Loss: 7275765042802468.0\n",
      "Predictions for NVDA: [[0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]\n",
      " [0.9999976]]\n",
      "True values for NVDA: [[5.4523784e-01 5.4933375e-01 5.3752786e-01 5.4017818e-01 2.4356000e+08\n",
      "  7.2022247e+01 7.1584558e-01]\n",
      " [1.4804297e+01 1.5213401e+01 1.4755156e+01 1.5091917e+01 2.0391200e+08\n",
      "  4.0185169e+01 3.8575837e-01]\n",
      " [5.8642654e+00 6.0385790e+00 5.8558469e+00 6.0326366e+00 2.2273200e+08\n",
      "  3.0506433e+01 2.8540906e-01]\n",
      " [1.7568777e+01 1.8170879e+01 1.6999626e+01 1.7662638e+01 7.6147398e+08\n",
      "  3.4146320e+01 3.2314748e-01]\n",
      " [1.6853846e+00 1.7018646e+00 1.6570981e+00 1.6669369e+00 4.2993200e+08\n",
      "  8.2240700e+01 8.2179064e-01]\n",
      " [3.7912378e+00 3.8643942e+00 3.7647030e+00 3.7783425e+00 6.0537203e+08\n",
      "  4.6284092e+01 4.4899204e-01]\n",
      " [4.9370378e-01 4.9730399e-01 4.9202371e-01 4.9298373e-01 1.9024000e+08\n",
      "  5.9841076e+01 5.8955109e-01]\n",
      " [3.0670900e+01 3.1270567e+01 3.0613934e+01 3.0982727e+01 3.5625299e+08\n",
      "  6.8996826e+01 6.8447798e-01]\n",
      " [8.0859119e-01 8.0859119e-01 7.8955984e-01 8.0078346e-01 1.4202400e+08\n",
      "  5.0310596e+01 4.9073893e-01]\n",
      " [3.7077916e-01 3.7617317e-01 3.6937204e-01 3.7359345e-01 2.3141200e+08\n",
      "  7.8472580e+01 7.8272283e-01]\n",
      " [5.1490273e+00 5.1963458e+00 4.7982273e+00 4.8425732e+00 9.5172000e+08\n",
      "  2.5524929e+01 2.3376077e-01]\n",
      " [4.3882080e+01 4.3915066e+01 4.2757591e+01 4.3013474e+01 4.8127699e+08\n",
      "  5.4731133e+01 5.3657109e-01]\n",
      " [3.3909222e-01 3.3909222e-01 3.2739940e-01 3.2808721e-01 6.0604403e+08\n",
      "  6.9665703e+01 6.9141299e-01]\n",
      " [3.4459481e-01 3.4505334e-01 3.3702886e-01 3.4023863e-01 4.7798800e+08\n",
      "  6.0062008e+01 5.9184170e-01]\n",
      " [5.0427866e-01 5.1632541e-01 5.0379682e-01 5.1584363e-01 3.5727600e+08\n",
      "  3.1638351e+01 2.9714480e-01]\n",
      " [3.6522925e-01 3.7417081e-01 3.6247799e-01 3.6316580e-01 5.1817600e+08\n",
      "  6.6292175e+01 6.5643615e-01]\n",
      " [3.6995783e-01 3.7019363e-01 3.6241248e-01 3.6335567e-01 3.8208000e+08\n",
      "  4.8818954e+01 4.7527355e-01]\n",
      " [2.3752506e-01 2.4165195e-01 2.3523235e-01 2.4142267e-01 8.1760397e+08\n",
      "  5.1378426e+01 5.0181019e-01]\n",
      " [2.5885572e+01 2.6009504e+01 2.5116001e+01 2.5600731e+01 4.3274701e+08\n",
      "  6.8281242e+01 6.7705888e-01]\n",
      " [4.5696849e-01 4.6676749e-01 4.5649049e-01 4.6676749e-01 2.6253600e+08\n",
      "  5.0819744e+01 4.9601775e-01]\n",
      " [3.4572549e+00 3.5228660e+00 3.4189818e+00 3.4870780e+00 3.9430000e+08\n",
      "  1.6346994e+01 1.3860379e-01]\n",
      " [3.2108578e-01 3.2387176e-01 3.1922844e-01 3.2201442e-01 2.1711600e+08\n",
      "  7.0232880e+01 6.9729340e-01]\n",
      " [5.3487760e-01 5.3800976e-01 5.3054070e-01 5.3391385e-01 1.5286400e+08\n",
      "  7.5526955e+01 7.5218248e-01]\n",
      " [1.2883330e+01 1.3374846e+01 1.2695516e+01 1.2856358e+01 5.3295299e+08\n",
      "  4.7759476e+01 4.6428889e-01]\n",
      " [2.4896538e+01 2.4905523e+01 2.4479284e+01 2.4833651e+01 2.3420400e+08\n",
      "  8.8511833e+01 8.8680989e-01]\n",
      " [5.3303518e+00 5.4926801e+00 5.3103080e+00 5.3647475e+00 4.9952400e+08\n",
      "  3.7069229e+01 3.5345224e-01]\n",
      " [1.1605607e+00 1.1757828e+00 1.1588420e+00 1.1725910e+00 2.1250800e+08\n",
      "  5.3212276e+01 5.2082360e-01]\n",
      " [4.5566979e-01 4.5781133e-01 4.4972110e-01 4.5162469e-01 2.1760800e+08\n",
      "  8.6399658e+01 8.6491084e-01]\n",
      " [1.3477055e+01 1.3610925e+01 1.3118820e+01 1.3135273e+01 4.1377600e+08\n",
      "  5.0905754e+01 4.9690953e-01]\n",
      " [3.8056836e-01 3.8198313e-01 3.7561673e-01 3.7821043e-01 3.3328800e+08\n",
      "  7.6190437e+01 7.5906152e-01]\n",
      " [3.4943771e-01 3.4967226e-01 3.4357467e-01 3.4709251e-01 1.9804400e+08\n",
      "  2.6451725e+01 2.4336979e-01]\n",
      " [4.6439919e+01 4.6474903e+01 4.5241467e+01 4.6267998e+01 5.1048800e+08\n",
      "  7.8520378e+01 7.8321838e-01]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue values for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:453\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    448\u001b[0m     warn_deprecated(\n\u001b[0;32m    449\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    452\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\matplotlib\\pyplot.py:3939\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   3919\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[0;32m   3920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[0;32m   3921\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3937\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3938\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PathCollection:\n\u001b[1;32m-> 3939\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3941\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3949\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3951\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3954\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3957\u001b[0m     sci(__ret)\n\u001b[0;32m   3958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:453\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    448\u001b[0m     warn_deprecated(\n\u001b[0;32m    449\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    452\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\lordw\\OneDrive\\Documents\\Coding_Projects\\AlgoTrAdInG\\venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:4900\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4898\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m   4899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m-> 4900\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4903\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   4904\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "    for ticker, loader in company_loaders.items():\n",
    "        for x_batch, y_batch in loader:\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Optionally, track or print predictions vs true values\n",
    "            if num_batches % 100 == 0:  # Print every 100 batches (optional)\n",
    "                print(f\"Test Loss after {num_batches} batches: {test_loss / num_batches}\")\n",
    "                \n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / num_batches\n",
    "print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# If you want to evaluate the performance for a specific ticker (company)\n",
    "ticker = \"NVDA\"  # Example: test for NVIDIA\n",
    "for x_batch, y_batch in company_loaders[ticker]:\n",
    "    # Perform prediction\n",
    "    y_pred = model(x_batch)\n",
    "    \n",
    "    # Convert to numpy for easier inspection\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    y_batch = y_batch.numpy()\n",
    "    \n",
    "    # Visualize or compare predictions and true values\n",
    "    print(f\"Predictions for {ticker}: {y_pred}\")\n",
    "    print(f\"True values for {ticker}: {y_batch}\")\n",
    "    plt.scatter(y_pred, y_batch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
